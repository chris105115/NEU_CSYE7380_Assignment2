{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10aa58f",
   "metadata": {},
   "source": [
    "# Developing a Simple Chatbot Using TensorFlow\n",
    "## A Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8341e6",
   "metadata": {},
   "source": [
    "source: \n",
    "- (04/2023) https://handsonai.medium.com/build-a-chat-bot-from-scratch-using-python-and-tensorflow-fd189bcfae45\n",
    "- (12/2023) https://handsonai.medium.com/developing-a-simple-chatbot-with-python-and-tensorflow-a-step-by-step-tutorial-0d35767e113b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762635d",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/bryanlimy/tf2-transformer-chatbot/blob/main/tf2_tpu_transformer_chatbot.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3fdaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a23d45",
   "metadata": {},
   "source": [
    "### 1. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7429810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create sample dataset\n",
    "conversations = [(\"Hello\", \"Hi there!\"),\n",
    "    (\"How are you?\", \"I'm doing well. Thank you.\"),\n",
    "    (\"What's your name?\", \"I'm Richard Wyckoff.\"),\n",
    "    (\"What do you do for a living?\", \"I'm a successful stock trader.\"),\n",
    "     # Add more conversational pairs as needed\n",
    "                ]\n",
    "\n",
    "# 2. Extract inputs and outputs\n",
    "inputs = [conversation[0] for conversation in conversations]  \n",
    "outputs = [conversation[1] for conversation in conversations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300393aa",
   "metadata": {},
   "source": [
    "inputs is a list with 4 elements.\n",
    "```Python\n",
    "inputs = ['Hello', 'How are you?', \"What's your name?\", 'What do you do for a living?']\n",
    "```\n",
    "outputs is a list with 4 elements.\n",
    "```Python\n",
    "outputs = ['Hi there!', \"I'm doing well. Thank you.\", \"I'm Richard Wyckoff.\", \"I'm a successful stock trader.\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dde00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Tokenizer for input and output texts\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(inputs)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(inputs)\n",
    "# input_sequences is a list with 4 elements. Each element is a list. \n",
    "# input_sequences = [[3], [4, 5, 1], [6, 7, 8], [9, 2, 1, 2, 10, 11, 12]]\n",
    "# e.g. you -> 1, do -> 2\n",
    "\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(outputs)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(outputs)\n",
    "# output_sequences is a list of lists with 4 elements. Each element respresents a sentence. \n",
    "# output_sequences = [[2, 3], [1, 4, 5, 6, 7], [1, 8, 9], [1, 10, 11, 12, 13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e472ef91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"i'm\": 1,\n",
       " 'hi': 2,\n",
       " 'there': 3,\n",
       " 'doing': 4,\n",
       " 'well': 5,\n",
       " 'thank': 6,\n",
       " 'you': 7,\n",
       " 'richard': 8,\n",
       " 'wyckoff': 9,\n",
       " 'a': 10,\n",
       " 'successful': 11,\n",
       " 'stock': 12,\n",
       " 'trader': 13}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e8b23",
   "metadata": {},
   "source": [
    "In the above step, each English word is tokenized into a number, using a word_index. You can check the value mapping by calling the following methods: \n",
    "``` Python\n",
    "input_tokenizer.word_index = {'you': 1, 'do': 2, 'hello': 3, 'how': 4, 'are': 5, \"what's\": 6, 'your': 7, 'name': 8, 'what': 9, 'for': 10, 'a': 11, 'living': 12}\n",
    "output_tokenizer.word_index = {\"i'm\": 1, 'hi': 2, 'there': 3, 'doing': 4, 'well': 5, 'thank': 6, 'you': 7,  'richard': 8, 'wyckoff': 9, 'a': 10, 'successful': 11, 'stock': 12, 'trader': 13}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b31797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Find maximum sequence lengths\n",
    "max_input_length = max(len(seq) for seq in input_sequences) # 7 \n",
    "max_output_length = max(len(seq) for seq in output_sequences) # 5\n",
    "# Note: input and output must to be of the same size. \n",
    "if max_input_length != max_output_length: \n",
    "        max_length = max(max_input_length, max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894e640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pad sequences\n",
    "X = pad_sequences(input_sequences, maxlen = max_length, padding='post')\n",
    "y = pad_sequences(output_sequences, maxlen = max_length, padding='post')\n",
    "# In padding, each word is padded into a 1 x 7 array: \n",
    "# Given input_sequences = [[3], [4, 5, 1], [6, 7, 8], [9, 2, 1, 2, 10, 11, 12]], \n",
    "# X = array([[ 3,  0,  0,  0,  0,  0,  0],\n",
    "#       [ 4,  5,  1,  0,  0,  0,  0],\n",
    "#       [ 6,  7,  8,  0,  0,  0,  0],\n",
    "#       [ 9,  2,  1,  2, 10, 11, 12]])\n",
    "# Given output_sequences = [[2, 3], [1, 4, 5, 6, 7], [1, 8, 9], [1, 10, 11, 12, 13]],\n",
    "# y = array([[ 2,  3,  0,  0,  0,  0,  0],\n",
    "#       [ 1,  4,  5,  6,  7,  0,  0],\n",
    "#       [ 1,  8,  9,  0,  0,  0,  0],\n",
    "#       [ 1, 10, 11, 12, 13,  0,  0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7624cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define model parameters\n",
    "vocab_size_input = len(input_tokenizer.word_index) + 1 # input uses 12 different words (vocabulary size)\n",
    "vocab_size_output = len(output_tokenizer.word_index) + 1 # output uses 15 different words (vocabulary size)\n",
    "embedding_dim = 64\n",
    "hidden_units = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8547ffd",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48962def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 7)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 7, 64)        832         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 7, 64)        896         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 100),        66000       ['embedding[0][0]']              \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 7, 100)       66000       ['embedding_1[0][0]',            \n",
      "                                                                  'lstm[0][1]',                   \n",
      "                                                                  'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7, 14)        1414        ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135,142\n",
      "Trainable params: 135,142\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 16s 16s/step - loss: 1.3186 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3146 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3106 - accuracy: 0.5714\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3065 - accuracy: 0.4286\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3021 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2975 - accuracy: 0.4286\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2925 - accuracy: 0.4286\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2870 - accuracy: 0.4286\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.2809 - accuracy: 0.4286\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2739 - accuracy: 0.3571\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2661 - accuracy: 0.3571\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2570 - accuracy: 0.3571\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2463 - accuracy: 0.3571\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2337 - accuracy: 0.3571\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2186 - accuracy: 0.3571\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2005 - accuracy: 0.3571\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1785 - accuracy: 0.3571\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1521 - accuracy: 0.3571\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1210 - accuracy: 0.3571\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0863 - accuracy: 0.3571\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0519 - accuracy: 0.3571\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0235 - accuracy: 0.3571\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0040 - accuracy: 0.3571\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9886 - accuracy: 0.3571\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9707 - accuracy: 0.3571\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9488 - accuracy: 0.3571\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9249 - accuracy: 0.3571\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9021 - accuracy: 0.3571\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8824 - accuracy: 0.4286\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8650 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8471 - accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8269 - accuracy: 0.5714\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.8049 - accuracy: 0.5714\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7835 - accuracy: 0.5714\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7642 - accuracy: 0.6429\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7466 - accuracy: 0.7143\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7289 - accuracy: 0.7143\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7093 - accuracy: 0.7143\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6879 - accuracy: 0.7857\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6658 - accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6445 - accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6236 - accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6016 - accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5791 - accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5577 - accuracy: 0.8571\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5377 - accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5171 - accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4953 - accuracy: 0.9286\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4734 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4522 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4313 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4108 - accuracy: 0.9286\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3914 - accuracy: 0.9286\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3723 - accuracy: 0.9286\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3534 - accuracy: 0.9286\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3352 - accuracy: 0.9286\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3182 - accuracy: 0.9286\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3020 - accuracy: 0.9286\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2866 - accuracy: 0.9286\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2721 - accuracy: 0.9286\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2580 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2444 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2318 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2200 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2092 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1994 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1901 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1813 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1729 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1648 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1572 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1501 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1433 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1370 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1308 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1249 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1193 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1138 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1087 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0993 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0949 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0906 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0865 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0787 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0750 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0679 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0646 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0613 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0582 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0523 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0495 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0420 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0377 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f651cc0f10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Model 2\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_length,))\n",
    "encoder_embedding = Embedding(vocab_size_input, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(hidden_units, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_length,))\n",
    "decoder_embedding = Embedding(vocab_size_output, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=False)(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_lstm)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Prepare the target data for training\n",
    "# The output needs to be reshaped for the decoder's output. We need to expand the dimensions of `y` for sparse categorical crossentropy.\n",
    "y = np.expand_dims(y, -1)\n",
    "\n",
    "# Train the model\n",
    "model.fit([X, X], y, batch_size=32, epochs=100)  # We are using X as decoder input for teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d076f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_response(input_text):\n",
    "    # Preprocess input text\n",
    "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_input_length, padding='post')\n",
    "    \n",
    "    # Predict response\n",
    "    # prediction = model.predict(input_seq) # model 1\n",
    "    prediction = model.predict([input_seq, input_seq]) # model 2\n",
    "    \n",
    "    # Convert prediction to text\n",
    "    predicted_seq = np.argmax(prediction, axis=-1)\n",
    "    response_words = [output_tokenizer.index_word.get(idx, '') for idx in predicted_seq[0]]\n",
    "    response_text = ' '.join(response_words)\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc1faa",
   "metadata": {},
   "source": [
    "#### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a61ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "Bot: i'm richard wyckoff wyckoff wyckoff wyckoff wyckoff\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot\n",
    "test_input = \"What's your name?\"\n",
    "response = predict_response(test_input)\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37794685",
   "metadata": {},
   "source": [
    "#### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c5ac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is today's date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Bot: i'm i'm i'm i'm i'm i'm i'm\n"
     ]
    }
   ],
   "source": [
    "# Start chatbot\n",
    "# while True:\n",
    "test_input = input('You: ')\n",
    "response = predict_response(test_input)\n",
    "print(f\"Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
